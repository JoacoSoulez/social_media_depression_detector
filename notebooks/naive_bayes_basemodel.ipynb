{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6054c8d2",
   "metadata": {},
   "source": [
    "# Bring data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 8,
>>>>>>> 650b48e076d671487376874d284ca9294e05f0ef
   "id": "526689db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import  MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 2,
>>>>>>> 650b48e076d671487376874d284ca9294e05f0ef
   "id": "93cdc1b3",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.../raw_data/depressive_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bj/ndgdqzgj6qgd9y613qph_m180000gn/T/ipykernel_1753/1332158325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#bring your data!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.../raw_data/depressive_tweets.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.../raw_data/depressive_tweets.csv'"
     ]
=======
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>name kati diagnos chronic depress panic disord...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tommcfli finger cross mexico hope well tour am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>kyzonxin damn mine differ realli involv money ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>learn okay talk depress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>vp ya well hope gettin feel bfh close play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       clean_tweets  label\n",
       "0           0  name kati diagnos chronic depress panic disord...      1\n",
       "1           1  tommcfli finger cross mexico hope well tour am...      0\n",
       "2           2  kyzonxin damn mine differ realli involv money ...      1\n",
       "3           3                            learn okay talk depress      1\n",
       "4           4         vp ya well hope gettin feel bfh close play      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 650b48e076d671487376874d284ca9294e05f0ef
    }
   ],
   "source": [
    "#bring your data!\n",
    "path = '/home/lucaspancotto/code/JoacoSoulez/mental_health_first_aid_evaluation/data/tweets_cleaned.csv'\n",
    "data = pd.read_csv(f'{path}')\n",
    "data.head()\n",
    "\n",
    "#/home/lucaspancotto/code/JoacoSoulez/mental_health_first_aid_evaluation/data/tweets_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b3d60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4851faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      2313\n",
       "clean_tweets    2308\n",
       "label           2313\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.label == 1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d287d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670627dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = data[data.label == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b64541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_data = data[data.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097e7972",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13451/2125965157.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_positive_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m2345\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5452\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5454\u001b[0;31m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/sample.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid weights: weights sum to zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     )\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "sample_positive_data = positive_data.sample( n  = 2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749314ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25781/2366412613.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sample_data = sample_positive_data.append(depression_data)\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_positive_data.append(depression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d271f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data =  sample_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538b89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'clean_tweets' #input('choose the text column of your data')\n",
    "X = data[f'{text_col}']\n",
    "\n",
    "target_col = 'label' #input('choose the target column of your data')\n",
    "y = data[f'{target_col}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ecfccc",
   "metadata": {},
   "source": [
    "# naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3649cb0",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fa522e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       name kati diagnos chronic depress panic disord...\n",
       "1       tommcfli finger cross mexico hope well tour am...\n",
       "2       kyzonxin damn mine differ realli involv money ...\n",
       "3                                 learn okay talk depress\n",
       "4              vp ya well hope gettin feel bfh close play\n",
       "                              ...                        \n",
       "4621                         taylor keith concert today l\n",
       "4622    quiz depress sex fuck cheerlead jacki nude eli...\n",
       "4623                                           danc blade\n",
       "4624    back hammer may th friend fallen legion weeken...\n",
       "4625    understand depress better luckili strong famil...\n",
       "Name: clean_tweets, Length: 4626, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid2 = TfidfVectorizer()\n",
    "nb2 = MultinomialNB()\n",
    "X = X.apply(str)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "760852be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('TfidfVectorizer', tfid2),\n",
    "    ('MultinomialNB()' , nb2)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ea64c7e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('TfidfVectorizer',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('MultinomialNB()',\n",
       "   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'TfidfVectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'MultinomialNB()': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'TfidfVectorizer__analyzer': 'word',\n",
       " 'TfidfVectorizer__binary': False,\n",
       " 'TfidfVectorizer__decode_error': 'strict',\n",
       " 'TfidfVectorizer__dtype': numpy.float64,\n",
       " 'TfidfVectorizer__encoding': 'utf-8',\n",
       " 'TfidfVectorizer__input': 'content',\n",
       " 'TfidfVectorizer__lowercase': True,\n",
       " 'TfidfVectorizer__max_df': 1.0,\n",
       " 'TfidfVectorizer__max_features': None,\n",
       " 'TfidfVectorizer__min_df': 1,\n",
       " 'TfidfVectorizer__ngram_range': (4, 5),\n",
       " 'TfidfVectorizer__norm': 'l2',\n",
       " 'TfidfVectorizer__preprocessor': None,\n",
       " 'TfidfVectorizer__smooth_idf': True,\n",
       " 'TfidfVectorizer__stop_words': None,\n",
       " 'TfidfVectorizer__strip_accents': None,\n",
       " 'TfidfVectorizer__sublinear_tf': False,\n",
       " 'TfidfVectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'TfidfVectorizer__tokenizer': None,\n",
       " 'TfidfVectorizer__use_idf': True,\n",
       " 'TfidfVectorizer__vocabulary': None,\n",
       " 'MultinomialNB()__alpha': 1.0,\n",
       " 'MultinomialNB()__class_prior': None,\n",
       " 'MultinomialNB()__fit_prior': True}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97bb330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_grid = {\n",
    "    'TfidfVectorizer__ngram_range': [(1,2) , (2,3), (3,4),(4, 5)],\n",
    "    'MultinomialNB()__alpha': [0.1 , 0.5 , 1.0]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84ce48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_recall= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc927baf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_recall.fit(X , y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3edd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for recall:  {'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best recall score:  0.98400345871163\n"
     ]
    }
   ],
   "source": [
    "print('the best params for recall: ',search_recall.best_params_)\n",
    "print('the best recall score: ' , search_recall.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ebcecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_accuracy= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7067f15",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_accuracy.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d33c2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for accuracy:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best accuracy score:  0.9118028534370947\n"
     ]
    }
   ],
   "source": [
    "print('the best params for accuracy: ',search_accuracy.best_params_)\n",
    "print('the best accuracy score: ' , search_accuracy.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ded16dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_precision= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='precision',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1375a6a9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='precision', verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_precision.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daf84eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for precision:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (4, 5)}\n",
      "the best precision score:  0.998095238095238\n"
     ]
    }
   ],
   "source": [
    "print('the best params for precision: ',search_precision.best_params_)\n",
    "print('the best precision score: ' , search_precision.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e22be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_f1= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c06d5dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_f1.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87172314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for f1:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best f1 score:  0.9169553240274925\n"
     ]
    }
   ],
   "source": [
    "print('the best params for f1: ',search_f1.best_params_)\n",
    "print('the best f1 score: ' , search_f1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca009c",
   "metadata": {},
   "source": [
    "para precision cambio el bestparams. hago un cross_validate para precision usando el bestparams de los otros scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4dbed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = search_accuracy.best_estimator_\n",
    "\n",
    "precision_cv = cross_val_score(best_accuracy,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='precision',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f9836d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for params  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}  is:  0.8624768653918675\n"
     ]
    }
   ],
   "source": [
    "print('precision for params ',search_accuracy.best_params_ ,' is: ',  np.mean(precision_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "325125af",
   "metadata": {},
   "source": [
    "# testeo y analisis del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a0ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid3 = TfidfVectorizer(ngram_range=(1,2))\n",
    "nb3 = MultinomialNB(alpha = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "619fab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c3e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector3 = tfid3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da103aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa poc',\n",
       " 'aaa',\n",
       " 'aaa shooter',\n",
       " 'aaaaaaaand',\n",
       " 'aaaaaaaand back',\n",
       " 'aaaaaah',\n",
       " 'aaaaaah http',\n",
       " 'aaaaaand',\n",
       " 'aaaaaand come',\n",
       " 'aaaaayyyyy',\n",
       " 'aaaaayyyyy momma',\n",
       " 'aaaayt',\n",
       " 'aaahhhh',\n",
       " 'aaahhhh hypocrisi',\n",
       " 'aaband',\n",
       " 'aaband cant',\n",
       " 'aaca',\n",
       " 'aacebad',\n",
       " 'aadawson',\n",
       " 'aadawson call',\n",
       " 'aah',\n",
       " 'aah well',\n",
       " 'aawjti',\n",
       " 'aawjti ocid',\n",
       " 'aaww',\n",
       " 'ab',\n",
       " 'ab challeng',\n",
       " 'ab gp',\n",
       " 'ab month',\n",
       " 'abandon',\n",
       " 'abandon heartbroken',\n",
       " 'abandon twitter',\n",
       " 'abat',\n",
       " 'abat depress',\n",
       " 'abatevintag',\n",
       " 'abatevintag fleurchipbowl',\n",
       " 'abatevintag list',\n",
       " 'abbelia',\n",
       " 'abbelia quot',\n",
       " 'abbey',\n",
       " 'abbey pook',\n",
       " 'abcacffcaeecd',\n",
       " 'abdulla',\n",
       " 'abdulla shaikh',\n",
       " 'abe',\n",
       " 'abe lincoln',\n",
       " 'abeautyhealthi',\n",
       " 'abeautyhealthi www',\n",
       " 'abel',\n",
       " 'abel love',\n",
       " 'abi',\n",
       " 'abi like',\n",
       " 'abil',\n",
       " 'abil accessal',\n",
       " 'abil manag',\n",
       " 'abil think',\n",
       " 'abl',\n",
       " 'abl accomplish',\n",
       " 'abl catch',\n",
       " 'abl dri',\n",
       " 'abl drink',\n",
       " 'abl get',\n",
       " 'abl handl',\n",
       " 'abl leav',\n",
       " 'abl pay',\n",
       " 'abl pull',\n",
       " 'abl run',\n",
       " 'abl sleepdepress',\n",
       " 'abl take',\n",
       " 'abl turn',\n",
       " 'abl wear',\n",
       " 'abl without',\n",
       " 'abl work',\n",
       " 'abl write',\n",
       " 'ableandgam',\n",
       " 'ableandgam boom',\n",
       " 'abnorm',\n",
       " 'abnorm function',\n",
       " 'aboard',\n",
       " 'aboard warren',\n",
       " 'abokssignatur',\n",
       " 'abokssignatur come',\n",
       " 'abort',\n",
       " 'abort child',\n",
       " 'abort consid',\n",
       " 'abotu',\n",
       " 'abotu everyth',\n",
       " 'aboutdepressionfact',\n",
       " 'aboutdepressionfact com',\n",
       " 'abraham',\n",
       " 'abraham lincoln',\n",
       " 'abroad',\n",
       " 'abroad depress',\n",
       " 'absenc',\n",
       " 'absenc cedar',\n",
       " 'absolut',\n",
       " 'absolut beauti',\n",
       " 'absolut reason',\n",
       " 'absolut rubbish',\n",
       " 'absolut shit',\n",
       " 'absolut terribl',\n",
       " 'absolut true',\n",
       " 'absorb',\n",
       " 'absorb wood',\n",
       " 'abstract',\n",
       " 'abstract utmsourc',\n",
       " 'abt',\n",
       " 'abt blk',\n",
       " 'abt depress',\n",
       " 'abt di',\n",
       " 'abt drug',\n",
       " 'abt recoveri',\n",
       " 'abt tell',\n",
       " 'abt vote',\n",
       " 'abt wan',\n",
       " 'abund',\n",
       " 'abund junk',\n",
       " 'abus',\n",
       " 'abus alochol',\n",
       " 'abus anxieti',\n",
       " 'abus anzac',\n",
       " 'abus bulli',\n",
       " 'abus depress',\n",
       " 'abus http',\n",
       " 'abus low',\n",
       " 'abus old',\n",
       " 'abus rape',\n",
       " 'abus support',\n",
       " 'abus survivor',\n",
       " 'abus teenag',\n",
       " 'abus ur',\n",
       " 'abzuy',\n",
       " 'abzuy oo',\n",
       " 'ac',\n",
       " 'ac arriv',\n",
       " 'ac mccann',\n",
       " 'ac uk',\n",
       " 'academ',\n",
       " 'academ pressur',\n",
       " 'acb',\n",
       " 'acb aph',\n",
       " 'acc',\n",
       " 'acc reader',\n",
       " 'accept',\n",
       " 'accept cuz',\n",
       " 'accept experi',\n",
       " 'accept otherwis',\n",
       " 'accept phase',\n",
       " 'accept refus',\n",
       " 'accept semi',\n",
       " 'accept stop',\n",
       " 'acceptancei',\n",
       " 'acceptancei still',\n",
       " 'access',\n",
       " 'access area',\n",
       " 'access drug',\n",
       " 'access mental',\n",
       " 'accessal',\n",
       " 'accessal climatechang',\n",
       " 'accessori',\n",
       " 'accessori lmao',\n",
       " 'accid',\n",
       " 'accid drop',\n",
       " 'accid suffer',\n",
       " 'accid ye',\n",
       " 'accident',\n",
       " 'accident made',\n",
       " 'accident quot',\n",
       " 'accident see',\n",
       " 'accident took',\n",
       " 'accnam',\n",
       " 'accnam guest',\n",
       " 'accompani',\n",
       " 'accompani mani',\n",
       " 'accomplish',\n",
       " 'accomplish one',\n",
       " 'accomplish sinc',\n",
       " 'accomplish someth',\n",
       " 'accord',\n",
       " 'accord cdcp',\n",
       " 'accord new',\n",
       " 'accord oceanup',\n",
       " 'accord someon',\n",
       " 'account',\n",
       " 'account depress',\n",
       " 'account pic',\n",
       " 'account queenxxlarray',\n",
       " 'account ship',\n",
       " 'account transpar',\n",
       " 'account twitter',\n",
       " 'account yay',\n",
       " 'account year',\n",
       " 'acctual',\n",
       " 'acctual remeb',\n",
       " 'accur',\n",
       " 'accur medium',\n",
       " 'accuradio',\n",
       " 'accuradio standard',\n",
       " 'accus',\n",
       " 'accus crap',\n",
       " 'ach',\n",
       " 'ach sleep',\n",
       " 'achi',\n",
       " 'achiev',\n",
       " 'achiev dream',\n",
       " 'achiev duplic',\n",
       " 'achiev get',\n",
       " 'achiev goal',\n",
       " 'achiv',\n",
       " 'achiv goal',\n",
       " 'acim',\n",
       " 'acim choic',\n",
       " 'aclzpbbwf',\n",
       " 'acn',\n",
       " 'acn caus',\n",
       " 'acn cure',\n",
       " 'acn depress',\n",
       " 'acortez',\n",
       " 'acortez love',\n",
       " 'acronym',\n",
       " 'acronym lol',\n",
       " 'across',\n",
       " 'across happi',\n",
       " 'across road',\n",
       " 'acryl',\n",
       " 'acryl charm',\n",
       " 'act',\n",
       " 'act better',\n",
       " 'act cool',\n",
       " 'act depress',\n",
       " 'act forsur',\n",
       " 'act injustic',\n",
       " 'act leav',\n",
       " 'act lesson',\n",
       " 'act like',\n",
       " 'act next',\n",
       " 'act pre',\n",
       " 'act self',\n",
       " 'act thought',\n",
       " 'act violent',\n",
       " 'act weird',\n",
       " 'action',\n",
       " 'action attent',\n",
       " 'action gain',\n",
       " 'action know',\n",
       " 'action say',\n",
       " 'action step',\n",
       " 'activ',\n",
       " 'activ cook',\n",
       " 'activ decreas',\n",
       " 'activ even',\n",
       " 'activ found',\n",
       " 'activ go',\n",
       " 'activ help',\n",
       " 'activ incid',\n",
       " 'activ late',\n",
       " 'activ lesser',\n",
       " 'activ lifestyl',\n",
       " 'activ like',\n",
       " 'activ moment',\n",
       " 'activ motiv',\n",
       " 'activ much',\n",
       " 'activ oomph',\n",
       " 'activ phase',\n",
       " 'activ pic',\n",
       " 'activ protect',\n",
       " 'activ recent',\n",
       " 'activ receptor',\n",
       " 'activ reject',\n",
       " 'activ robust',\n",
       " 'activ serotonin',\n",
       " 'activ thank',\n",
       " 'activelinc',\n",
       " 'activelinc bikeabl',\n",
       " 'actor',\n",
       " 'actor regret',\n",
       " 'actor symptom',\n",
       " 'actor vern',\n",
       " 'actress',\n",
       " 'actress said',\n",
       " 'actscen',\n",
       " 'actscen watch',\n",
       " 'actual',\n",
       " 'actual bad',\n",
       " 'actual believ',\n",
       " 'actual bipolar',\n",
       " 'actual catapult',\n",
       " 'actual chemic',\n",
       " 'actual chosen',\n",
       " 'actual coffe',\n",
       " 'actual commit',\n",
       " 'actual cure',\n",
       " 'actual definit',\n",
       " 'actual depress',\n",
       " 'actual edit',\n",
       " 'actual everyon',\n",
       " 'actual father',\n",
       " 'actual feel',\n",
       " 'actual fight',\n",
       " 'actual figur',\n",
       " 'actual happi',\n",
       " 'actual help',\n",
       " 'actual high',\n",
       " 'actual imbal',\n",
       " 'actual interact',\n",
       " 'actual liter',\n",
       " 'actual lucki',\n",
       " 'actual make',\n",
       " 'actual news',\n",
       " 'actual one',\n",
       " 'actual pictur',\n",
       " 'actual poor',\n",
       " 'actual real',\n",
       " 'actual realis',\n",
       " 'actual realli',\n",
       " 'actual say',\n",
       " 'actual sleep',\n",
       " 'actual someth',\n",
       " 'actual start',\n",
       " 'actual struggl',\n",
       " 'actual take',\n",
       " 'actual tast',\n",
       " 'actual thought',\n",
       " 'actual understand',\n",
       " 'actual vile',\n",
       " 'actual ye',\n",
       " 'acut',\n",
       " 'acut depress',\n",
       " 'ad',\n",
       " 'ad air',\n",
       " 'ad bonu',\n",
       " 'ad diagnost',\n",
       " 'ad difficulti',\n",
       " 'ad hominem',\n",
       " 'ad jeremywright',\n",
       " 'ad myspac',\n",
       " 'ad pay',\n",
       " 'ad tthat',\n",
       " 'ad video',\n",
       " 'ad watch',\n",
       " 'ad yr',\n",
       " 'adam',\n",
       " 'adam eve',\n",
       " 'adam fli',\n",
       " 'adamfyr',\n",
       " 'adamfyr absolut',\n",
       " 'adamhoward',\n",
       " 'adamhoward serious',\n",
       " 'adammessing',\n",
       " 'adammessing exactli',\n",
       " 'adammilo',\n",
       " 'adammilo depress',\n",
       " 'adamsilvera',\n",
       " 'adamsilvera book',\n",
       " 'adapaavi',\n",
       " 'adapaavi crucifir',\n",
       " 'adblock',\n",
       " 'adblock er',\n",
       " 'add',\n",
       " 'add anxieti',\n",
       " 'add babi',\n",
       " 'add cours',\n",
       " 'add depress',\n",
       " 'add emoji',\n",
       " 'add everyon',\n",
       " 'add frostpunk',\n",
       " 'add mental',\n",
       " 'add pain',\n",
       " 'add pl',\n",
       " 'add rice',\n",
       " 'add score',\n",
       " 'add sever',\n",
       " 'add whatev',\n",
       " 'addam',\n",
       " 'addam famili',\n",
       " 'adderal',\n",
       " 'addict',\n",
       " 'addict abus',\n",
       " 'addict anxieti',\n",
       " 'addict arent',\n",
       " 'addict blend',\n",
       " 'addict chronic',\n",
       " 'addict coffe',\n",
       " 'addict crimin',\n",
       " 'addict daili',\n",
       " 'addict depress',\n",
       " 'addict domest',\n",
       " 'addict dont',\n",
       " 'addict eat',\n",
       " 'addict featur',\n",
       " 'addict game',\n",
       " 'addict good',\n",
       " 'addict hotlin',\n",
       " 'addict inspir',\n",
       " 'addict like',\n",
       " 'addict one',\n",
       " 'addict psychosi',\n",
       " 'addict reduc',\n",
       " 'addict select',\n",
       " 'addict take',\n",
       " 'addict think',\n",
       " 'addit',\n",
       " 'addit exercis',\n",
       " 'addit famili',\n",
       " 'addit forget',\n",
       " 'addit imag',\n",
       " 'addn',\n",
       " 'addn locat',\n",
       " 'addon',\n",
       " 'addon featur',\n",
       " 'address',\n",
       " 'address abe',\n",
       " 'address bipolar',\n",
       " 'address mental',\n",
       " 'address pendem',\n",
       " 'address question',\n",
       " 'address real',\n",
       " 'adfeefb',\n",
       " 'adfeefb utmterm',\n",
       " 'adhd',\n",
       " 'adhd affect',\n",
       " 'adhd depress',\n",
       " 'adhd get',\n",
       " 'adhd horribl',\n",
       " 'adhd one',\n",
       " 'adhd parent',\n",
       " 'adhd sad',\n",
       " 'adhd stress',\n",
       " 'adio',\n",
       " 'adium',\n",
       " 'adium im',\n",
       " 'adjust',\n",
       " 'adjust know',\n",
       " 'adkinsk',\n",
       " 'adkinsk sheffieldpsi',\n",
       " 'adm',\n",
       " 'adm get',\n",
       " 'administr',\n",
       " 'administr commun',\n",
       " 'administr lead',\n",
       " 'admir',\n",
       " 'admir jackson',\n",
       " 'admir share',\n",
       " 'admir ur',\n",
       " 'admit',\n",
       " 'admit ate',\n",
       " 'admit depress',\n",
       " 'admit fault',\n",
       " 'admit higher',\n",
       " 'admit hospit',\n",
       " 'admit other',\n",
       " 'admit patient',\n",
       " 'admit problem',\n",
       " 'admit problemy',\n",
       " 'admit say',\n",
       " 'adnankhun',\n",
       " 'adnankhun nurulnxha',\n",
       " 'adobespark',\n",
       " 'adobespark creat',\n",
       " 'adoizv',\n",
       " 'adoizv overland',\n",
       " 'adolesc',\n",
       " 'adolesc accord',\n",
       " 'adolesc depart',\n",
       " 'adolesc depress',\n",
       " 'adolesc gay',\n",
       " 'adolesc higher',\n",
       " 'adolesc make',\n",
       " 'adolesc may',\n",
       " 'adolesc solut',\n",
       " 'adolesc univers',\n",
       " 'adolesc year',\n",
       " 'adopt',\n",
       " 'adopt band',\n",
       " 'adopt lgbqt',\n",
       " 'ador',\n",
       " 'ador af',\n",
       " 'ador mind',\n",
       " 'ador sound',\n",
       " 'adorable',\n",
       " 'adorable swear',\n",
       " 'adragonswing',\n",
       " 'adragonswing absolut',\n",
       " 'adreanainlb',\n",
       " 'adreanainlb mairinmurphi',\n",
       " 'adrian',\n",
       " 'adrian music',\n",
       " 'adrian row',\n",
       " 'adrianlynch',\n",
       " 'adrianlynch oop',\n",
       " 'adrianvziegl',\n",
       " 'adrianvziegl hi',\n",
       " 'adrift',\n",
       " 'adrift frequent',\n",
       " 'aduc',\n",
       " 'aduc alfqrd',\n",
       " 'adul',\n",
       " 'adul demon',\n",
       " 'adult',\n",
       " 'adult addit',\n",
       " 'adult autism',\n",
       " 'adult comic',\n",
       " 'adult depress',\n",
       " 'adult diagnosi',\n",
       " 'adult happi',\n",
       " 'adult life',\n",
       " 'adult men',\n",
       " 'adult nesdo',\n",
       " 'adult realli',\n",
       " 'aduval',\n",
       " 'aduval go',\n",
       " 'adv',\n",
       " 'adv larger',\n",
       " 'advanc',\n",
       " 'advanc swear',\n",
       " 'advantag',\n",
       " 'advantag exercisework',\n",
       " 'advantag peopl',\n",
       " 'adventuretravel',\n",
       " 'adventuretravel travel',\n",
       " 'advers',\n",
       " 'advers ld',\n",
       " 'advers like',\n",
       " 'advertis',\n",
       " 'advertis chase',\n",
       " 'advertis class',\n",
       " 'advertis promot',\n",
       " 'advertis servic',\n",
       " 'advic',\n",
       " 'advic everi',\n",
       " 'advic hrer',\n",
       " 'advic know',\n",
       " 'advic need',\n",
       " 'advic offer',\n",
       " 'advic relaps',\n",
       " 'advic suffer',\n",
       " 'advisor',\n",
       " 'advisor depress',\n",
       " 'advisor one',\n",
       " 'advoc',\n",
       " 'advoc caus',\n",
       " 'advoc pic',\n",
       " 'adweek',\n",
       " 'adweek friend',\n",
       " 'adweek pic',\n",
       " 'adweek qyr',\n",
       " 'aecpa',\n",
       " 'aecpa adreanainlb',\n",
       " 'aemtrtvp',\n",
       " 'aerliss',\n",
       " 'aerliss find',\n",
       " 'aerob',\n",
       " 'aerob strengthen',\n",
       " 'aesthet',\n",
       " 'aesthet depress',\n",
       " 'af',\n",
       " 'af depress',\n",
       " 'af though',\n",
       " 'af twice',\n",
       " 'af via',\n",
       " 'affect',\n",
       " 'affect abil',\n",
       " 'affect anxieti',\n",
       " 'affect anyon',\n",
       " 'affect belief',\n",
       " 'affect ch',\n",
       " 'affect could',\n",
       " 'affect depress',\n",
       " 'affect disord',\n",
       " 'affect health',\n",
       " 'affect level',\n",
       " 'affect million',\n",
       " 'affect oughtibridg',\n",
       " 'affect peopl',\n",
       " 'affect person',\n",
       " 'affect product',\n",
       " 'affect sever',\n",
       " 'affect sleep',\n",
       " 'affect write',\n",
       " 'affilli',\n",
       " 'affilli site',\n",
       " 'affirm',\n",
       " 'affirm big',\n",
       " 'affirm feelgoodart',\n",
       " 'affirm video',\n",
       " 'afford',\n",
       " 'afford bu',\n",
       " 'afford cure',\n",
       " 'afford fuck',\n",
       " 'afford sock',\n",
       " 'afford time',\n",
       " 'afloat',\n",
       " 'afraid',\n",
       " 'afraid admit',\n",
       " 'afraid confid',\n",
       " 'afraid tri',\n",
       " 'afraid would',\n",
       " 'africa',\n",
       " 'africa billion',\n",
       " 'africa tech',\n",
       " 'african',\n",
       " 'african american',\n",
       " 'african suffer',\n",
       " 'afsfh',\n",
       " 'afsfh statu',\n",
       " 'afsfh stress',\n",
       " 'afternoon',\n",
       " 'afternoon gym',\n",
       " 'afternoon nap',\n",
       " 'afterschool',\n",
       " 'afterschool tutor',\n",
       " 'aftershock',\n",
       " 'aftershock wake',\n",
       " 'afterward',\n",
       " 'afterward confirm',\n",
       " 'afterward make',\n",
       " 'agajikxz',\n",
       " 'agajikxz pic',\n",
       " 'age',\n",
       " 'age among',\n",
       " 'age bit',\n",
       " 'age blame',\n",
       " 'age depress',\n",
       " 'age edward',\n",
       " 'age fight',\n",
       " 'age gender',\n",
       " 'age geograph',\n",
       " 'age good',\n",
       " 'age join',\n",
       " 'age peopl',\n",
       " 'age process',\n",
       " 'age rais',\n",
       " 'age research',\n",
       " 'age soo',\n",
       " 'age wealth',\n",
       " 'age wrinkl',\n",
       " 'age yoga',\n",
       " 'agebettersheffield',\n",
       " 'agebettersheffield http',\n",
       " 'agenc',\n",
       " 'agenc want',\n",
       " 'agenciaajn',\n",
       " 'agenciaajn com',\n",
       " 'agent',\n",
       " 'aggreg',\n",
       " 'aggreg im',\n",
       " 'aggress',\n",
       " 'aggress argument',\n",
       " 'aggress behavior',\n",
       " 'aggress medic',\n",
       " 'aggress never',\n",
       " 'agh',\n",
       " 'agh sorri',\n",
       " 'agit',\n",
       " 'agit nervous',\n",
       " 'agnosticquest',\n",
       " 'agnosticquest onfiremiss',\n",
       " 'ago',\n",
       " 'ago destroy',\n",
       " 'ago didnt',\n",
       " 'ago excus',\n",
       " 'ago experienc',\n",
       " 'ago gladli',\n",
       " 'ago go',\n",
       " 'ago gon',\n",
       " 'ago great',\n",
       " 'ago hah',\n",
       " 'ago happen',\n",
       " 'ago happi',\n",
       " 'ago head',\n",
       " 'ago hockey',\n",
       " 'ago meme',\n",
       " 'ago scratch',\n",
       " 'ago speak',\n",
       " 'ago took',\n",
       " 'agoni',\n",
       " 'agoraphobiadepressionptsdanxietysicknotweaki',\n",
       " 'agoraphobiadepressionptsdanxietysicknotweaki coupl',\n",
       " 'agre',\n",
       " 'agre ad',\n",
       " 'agre although',\n",
       " 'agre anyway',\n",
       " 'agre bulli',\n",
       " 'agre cbetta',\n",
       " 'agre chang',\n",
       " 'agre disagre',\n",
       " 'agre emoji',\n",
       " 'agre final',\n",
       " 'agre let',\n",
       " 'agre luna',\n",
       " 'agre nanotissera',\n",
       " 'agre new',\n",
       " 'agre opinion',\n",
       " 'agre sorri',\n",
       " 'agreement',\n",
       " 'agreement collect',\n",
       " 'agynaathavaasi',\n",
       " 'agynaathavaasi core',\n",
       " 'ah',\n",
       " 'ah feel',\n",
       " 'ah june',\n",
       " 'ah look',\n",
       " 'ah never',\n",
       " 'ah oki',\n",
       " 'ah tast',\n",
       " 'ah thank',\n",
       " 'ah ty',\n",
       " 'ah vision',\n",
       " 'ah well',\n",
       " 'aha',\n",
       " 'aha good',\n",
       " 'aha manag',\n",
       " 'aha miley',\n",
       " 'aha settl',\n",
       " 'ahaha',\n",
       " 'ahav',\n",
       " 'ahav beauti',\n",
       " 'ahead',\n",
       " 'ahead admit',\n",
       " 'ahead dark',\n",
       " 'ahead est',\n",
       " 'ahead ur',\n",
       " 'ahead ya',\n",
       " 'aherman',\n",
       " 'aherman depress',\n",
       " 'ahh',\n",
       " 'ahh blame',\n",
       " 'ahh brilliant',\n",
       " 'ahh like',\n",
       " 'ahh suck',\n",
       " 'ahh tube',\n",
       " 'ahh well',\n",
       " 'ahhaha',\n",
       " 'ahhaha mr',\n",
       " 'ahhh',\n",
       " 'ahhh feel',\n",
       " 'ahhh get',\n",
       " 'ahhh trailer',\n",
       " 'ahhhh',\n",
       " 'ahhhh daluckym',\n",
       " 'ahhhh herkonwpgc',\n",
       " 'ahhhhh',\n",
       " 'ahhhhh use',\n",
       " 'ahja',\n",
       " 'ahja lieb',\n",
       " 'ahmedaniy',\n",
       " 'ahmedaniy toddtrott',\n",
       " 'ahold',\n",
       " 'ahold take',\n",
       " 'ahoy',\n",
       " 'ahrenfelt',\n",
       " 'ahrenfelt chanc',\n",
       " 'ahukewirjnbedtaahvgkgmkhubxqauieygd',\n",
       " 'ahukewirjnbedtaahvgkgmkhubxqauieygd biw',\n",
       " 'ai',\n",
       " 'ai ffyu',\n",
       " 'aid',\n",
       " 'aid content',\n",
       " 'aightindiaaaaaa',\n",
       " 'aightindiaaaaaa give',\n",
       " 'aiinb',\n",
       " 'aikenstandard',\n",
       " 'aikenstandard com',\n",
       " 'aileeneduyan',\n",
       " 'aileeneduyan ahhaha',\n",
       " 'aim',\n",
       " 'aim anyon',\n",
       " 'aim help',\n",
       " 'aim reduc',\n",
       " 'aim went',\n",
       " 'aimeexzarnow',\n",
       " 'aimeexzarnow definit',\n",
       " 'aimstah',\n",
       " 'aimstah lol',\n",
       " 'aint',\n",
       " 'aint babi',\n",
       " 'aint depress',\n",
       " 'aint noth',\n",
       " 'air',\n",
       " 'air con',\n",
       " 'air lol',\n",
       " 'air sinc',\n",
       " 'air today',\n",
       " 'air week',\n",
       " 'airi',\n",
       " 'airplan',\n",
       " 'airplan departur',\n",
       " 'airplan depress',\n",
       " 'airplan earli',\n",
       " 'airplan earlier',\n",
       " 'airplan intens',\n",
       " 'airport',\n",
       " 'airport bu',\n",
       " 'airport pick',\n",
       " 'airtim',\n",
       " 'airtim pro',\n",
       " 'aish',\n",
       " 'aish achiev',\n",
       " 'aish fan',\n",
       " 'aja',\n",
       " 'ajackielarsen',\n",
       " 'ajackielarsen greatdaneuj',\n",
       " 'ajackielarsen idk',\n",
       " 'ajackielarsen nemosanartist',\n",
       " 'ajackielarsen statu',\n",
       " 'ajatt',\n",
       " 'ajatt hey',\n",
       " 'ajatt post',\n",
       " 'ajbrzski',\n",
       " 'ajbrzski angrierthanmost',\n",
       " 'ajhi',\n",
       " 'ajhi urekyewflst',\n",
       " 'ajit',\n",
       " 'ajit intrigu',\n",
       " 'ajm',\n",
       " 'ajm whole',\n",
       " 'ajp',\n",
       " 'ajp doi',\n",
       " 'ajp exercisework',\n",
       " 'ajp includ',\n",
       " 'ajp platform',\n",
       " 'ajp psychiatryonlin',\n",
       " 'ajsgmajc',\n",
       " 'ajsgmajc donnawithrow',\n",
       " 'aka',\n",
       " 'aka conciqu',\n",
       " 'aka democrat',\n",
       " 'aka depress',\n",
       " 'aka penelop',\n",
       " 'akcounsellor',\n",
       " 'akcounsellor know',\n",
       " 'akfryoo',\n",
       " 'akgovsarahpalin',\n",
       " 'akgovsarahpalin normal',\n",
       " 'ako',\n",
       " 'ako ng',\n",
       " 'ako within',\n",
       " 'akoomatsu',\n",
       " 'akoomatsu statu',\n",
       " 'akp',\n",
       " 'akp turkey',\n",
       " 'akyarni',\n",
       " 'akyarni sure',\n",
       " 'al',\n",
       " 'al walk',\n",
       " 'ala',\n",
       " 'ala depress',\n",
       " 'alan',\n",
       " 'alan tate',\n",
       " 'alanahhead',\n",
       " 'alancostello',\n",
       " 'alancostello necessarili',\n",
       " 'alarm',\n",
       " 'alarm rise',\n",
       " 'alawin',\n",
       " 'alawin alway',\n",
       " 'albani',\n",
       " 'albani go',\n",
       " 'albeit',\n",
       " 'albeit part',\n",
       " 'albeit second',\n",
       " 'albino',\n",
       " 'albino structur',\n",
       " 'album',\n",
       " 'album absolut',\n",
       " 'album artist',\n",
       " 'album bateman',\n",
       " 'album concern',\n",
       " 'album coupl',\n",
       " 'album ever',\n",
       " 'album forgotten',\n",
       " 'album franc',\n",
       " 'album hanniundnanni',\n",
       " 'album hondura',\n",
       " 'album like',\n",
       " 'album line',\n",
       " 'album listen',\n",
       " 'album mania',\n",
       " 'album might',\n",
       " 'album tomorrow',\n",
       " 'albuquerqu',\n",
       " 'albuquerqu north',\n",
       " 'alcohol',\n",
       " 'alcohol abus',\n",
       " 'alcohol addict',\n",
       " 'alcohol cocain',\n",
       " 'alcohol cross',\n",
       " 'alcohol drug',\n",
       " 'alcohol easi',\n",
       " 'alcohol marijuana',\n",
       " 'alcohol never',\n",
       " 'alcohol poison',\n",
       " 'alcohol potenti',\n",
       " 'alcohol ur',\n",
       " 'alcoholharmoni',\n",
       " 'alcoholharmoni omg',\n",
       " 'aldridtl',\n",
       " 'aldridtl thank',\n",
       " 'alecsharp',\n",
       " 'alecsharp find',\n",
       " 'alert',\n",
       " 'alert activ',\n",
       " 'alert suddenli',\n",
       " 'alex',\n",
       " 'alex case',\n",
       " 'alex watch',\n",
       " 'alexa',\n",
       " 'alexa ask',\n",
       " 'alexaam',\n",
       " 'alexaam haha',\n",
       " 'alexagor',\n",
       " 'alexagor quot',\n",
       " 'alexalbrecht',\n",
       " 'alexalbrecht depend',\n",
       " 'alexand',\n",
       " 'alexanderkalma',\n",
       " 'alexanderkalma futur',\n",
       " 'alexbrannan',\n",
       " 'alexbrannan level',\n",
       " 'alexbwel',\n",
       " 'alexbwel would',\n",
       " 'alexi',\n",
       " 'alexi ftw',\n",
       " 'alexiaaa',\n",
       " 'alexiaaa ye',\n",
       " 'alfqrd',\n",
       " 'alharri',\n",
       " 'alharri haha',\n",
       " 'alibabaotu',\n",
       " 'alibabaotu lololol',\n",
       " 'alic',\n",
       " 'alic fan',\n",
       " 'alic wonderland',\n",
       " 'alien',\n",
       " 'alien forc',\n",
       " 'alien world',\n",
       " 'alinichol',\n",
       " 'alinichol think',\n",
       " 'alisawoodard',\n",
       " 'alisawoodard swantowski',\n",
       " 'aliv',\n",
       " 'aliv care',\n",
       " 'aliv emoji',\n",
       " 'aliv life',\n",
       " 'aliv matthaig',\n",
       " 'aliv pic',\n",
       " 'aliv rel',\n",
       " 'alka',\n",
       " 'alka seltza',\n",
       " 'alkerm',\n",
       " 'alkerm depress',\n",
       " 'allah',\n",
       " 'allay',\n",
       " 'allay depress',\n",
       " 'allay malais',\n",
       " 'allbloominwrong',\n",
       " 'allbloominwrong said',\n",
       " 'allen',\n",
       " 'allen adam',\n",
       " 'allen version',\n",
       " 'allerg',\n",
       " 'allerg friend',\n",
       " 'allergi',\n",
       " 'allergi depress',\n",
       " 'allergi leav',\n",
       " 'allergi mental',\n",
       " 'allevi',\n",
       " 'allevi anxieti',\n",
       " 'allevi depress',\n",
       " 'allevi problem',\n",
       " 'allevin',\n",
       " 'allevin sadli',\n",
       " 'alley',\n",
       " 'alley follow',\n",
       " 'alli',\n",
       " 'alli depress',\n",
       " 'alli today',\n",
       " 'allison',\n",
       " 'allison watch',\n",
       " 'alllll',\n",
       " 'alllll depress',\n",
       " 'allow',\n",
       " 'allow depress',\n",
       " 'allow euthanasia',\n",
       " 'allow feel',\n",
       " 'allow see',\n",
       " 'allow wallow',\n",
       " 'allow win',\n",
       " 'allthesedoubt',\n",
       " 'allthesedoubt interest',\n",
       " 'allthesimpl',\n",
       " 'allthesimpl visiontwit',\n",
       " 'alluresha',\n",
       " 'alluresha statu',\n",
       " 'allysarmydoc',\n",
       " 'allysarmydoc wait',\n",
       " 'almighti',\n",
       " 'almighti joe',\n",
       " 'almost',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60138d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.fit(vector3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0abcf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.predict(vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b567109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.8360094 , -10.8360094 , -10.82574617, ..., -10.98479442,\n",
       "        -10.98479442, -10.98479442]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "021fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(nb3.coef_[0], index = tfid3.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49bc93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depress           -6.087078\n",
       "com               -7.141977\n",
       "http              -7.287310\n",
       "twitter           -7.306369\n",
       "twitter com       -7.312442\n",
       "anxieti           -7.336571\n",
       "emoji             -7.516593\n",
       "pic twitter       -7.631710\n",
       "pic               -7.642546\n",
       "face              -7.856854\n",
       "like              -7.894928\n",
       "depress anxieti   -8.034378\n",
       "anxieti depress   -8.039461\n",
       "http twitter      -8.044223\n",
       "statu             -8.054516\n",
       "feel              -8.078257\n",
       "go                -8.112839\n",
       "get               -8.148739\n",
       "peopl             -8.154361\n",
       "help              -8.181407\n",
       "know              -8.193267\n",
       "life              -8.201921\n",
       "www               -8.225839\n",
       "cure              -8.236301\n",
       "depress http      -8.240823\n",
       "one               -8.298598\n",
       "mental            -8.346886\n",
       "thing             -8.368970\n",
       "http www          -8.374599\n",
       "health            -8.398429\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: y cannot be None\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: y cannot be None\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: y cannot be None\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: y cannot be None\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: y cannot be None\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "coefs.sort_values(ascending=False).head(30)\n",
    "\n",
    "#agregar palabras a las stopwords , por ejemplo 'twitter', 'com'\n",
    "#agregar precision, accuracy, recall , f1 , etc\n",
    "#min, max \n",
    "#max_features:overfitting? ~ regularizacion\n",
    "\n",
    "#to do: traduccion?\n",
    "#red de deeplearning para buscar mejor score\n",
    "#testeos\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dd61f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56ba23fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb3_model.sav']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(nb3, 'nb3_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8dbe069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfid3_vectorizer.sav']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfid3 , 'tfid3_vectorizer.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3ee0efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'BorradorClean (1).ipynb'\r\n",
      " LucasPancotto_notebook.ipynb\r\n",
      " naive_bayes_basemodel.ipynb\r\n",
      " nb3_model.sav\r\n",
      " test_depressionreddit_simple_naivebayes_model.ipynb\r\n",
      " tfid3_vectorizer.sav\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
